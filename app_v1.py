
from tqdm import tqdm
import os
import google.generativeai as genai
from dotenv import load_dotenv
from pdf2image import convert_from_path
from PIL import Image
import time
import streamlit as st
import google.generativeai as genai
from pathlib import Path
from pdf2image import convert_from_path
from pathlib import Path
import os
import fitz 
from pathlib import Path
import time
from io import BytesIO
import os
from PIL import Image
import os
import google.generativeai as genai
import json
# st.title("DD with gemini-2.0-flash")
genai.configure(api_key=GOOGLE_API_KEY)


# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«Geminiã®APIã‚­ãƒ¼ã®å…¥åŠ›æ¬„ã‚’è¨­ã‘ã‚‹
with st.sidebar:
    gemini_api_key = st.text_input("Gemini API Key", key="chatbot_api_key", type="password")
    "[Get an Gemini API key](https://aistudio.google.com/app/apikey)"
    "[View the source code](https://github.com/danishi/streamlit-gemini-chatbot)"

st.title("âœ¨ Gemini Chatbot")
st.caption("ğŸš€ A Streamlit chatbot powered by Gemini")

pdf_folder = Path("/workspaces/streamit_sample_app/data/input")
pdf_folder.mkdir(parents=True, exist_ok=True)

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
uploaded_pdf = st.file_uploader("PDFã‚’èª­ã¿è¾¼ã‚€", type=["pdf"])

if uploaded_pdf is not None:
    # ä¿å­˜ãƒ‘ã‚¹ã‚’è¨­å®š
    save_path = pdf_folder / uploaded_pdf.name
    
    # PDFã‚’ä¿å­˜
    with open(save_path, "wb") as f:
        f.write(uploaded_pdf.getbuffer())  # PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
image_folder = Path("/workspaces/streamit_sample_app/data/output")  # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
image_folder.mkdir(exist_ok=True)  # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆ

# Open the first PDF file from the PDF folder
pdf_files = list(pdf_folder.glob("*.pdf"))  # Use the first PDF for example
if pdf_files:
    for pdf_file in pdf_files:
        pdf_file = pdf_file
        doc = fitz.open(pdf_file)

        # PDFã”ã¨ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        pdf_subfolder = image_folder / "image" / pdf_file.stem
        pdf_subfolder.mkdir(parents=True, exist_ok=True)

        # å„ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            pix = page.get_pixmap()
            image_path = pdf_subfolder / f"{pdf_file.stem}_{page_num + 1}.jpg"
            pix.save(image_path)

        doc.close()
        st.success(f"ç”»åƒãŒ {pdf_subfolder} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    pdf_file = pdf_file
else:
    st.warning("PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãŒãªã‘ã‚Œã°åˆæœŸåŒ–
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
for message in st.session_state.chat_history:
    st.chat_message(message["role"]).write(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒé€ä¿¡ã•ã‚ŒãŸéš›ã«å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†
if prompt := st.chat_input("How can I help you?"):

    # APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
    if not gemini_api_key:
        st.info("Please add your [Gemini API key](https://aistudio.google.com/app/apikey) to continue.")
        st.stop()

    # ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    genai.configure(api_key=gemini_api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    # ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ ã—ç”»é¢è¡¨ç¤º
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
    messages = []
    for message in st.session_state.chat_history:
        messages.append({
            "role": message["role"] if message["role"] == "user" else "model",
            'parts': message["content"]
        })

    # Geminiã¸å•ã„åˆã‚ã›ã‚’è¡Œã†
    response = model.generate_content(messages)

    # Geminiã®è¿”ç­”ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ ã—ç”»é¢è¡¨ç¤º
    st.session_state.chat_history.append({"role": "assistant", "content": response.text})
    st.chat_message("assistant").write(response.text)

import os
import google.generativeai as genai



# Create the model
generation_config = {
  "temperature": 1,
  "top_p": 0.95,
  "top_k": 40,
  "max_output_tokens": 8192,
  "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(
  model_name="gemini-2.0-flash",
  generation_config=generation_config,
)

chat_session = model.start_chat(
  history=[
  ]
)

response = chat_session.send_message("INSERT_INPUT_HERE")

response.text.strip() if response and response.text else None
def process_with_gemini(image):
    """
    Send the page image to Gemini and extract structured Markdown content.
    """
    model = genai.GenerativeModel("gemini-2.0-flash")

    # API Rate Limiting
    time.sleep(2)
    
    prompt = """
    Extract all contents from the provided PDF page image and format it as Markdown in Japanese.  
    Do NOT extract repeated headers that appear on every page.  

    ### Guidelines: 
    - Structure text properly into headings, paragraphs, lists, and bullet points.  
    - Tables: If a table exists, extract it accurately and format it using proper Markdown table syntax.  
    - Graphs & Figures: If a graph or figure contains important numerical data, summarize key insights and represent them as a table if possible.  
    - Equations: If mathematical formulas appear, format them using LaTeX syntax within Markdown.  
    """

    response = model.generate_content([prompt, image])
    return response.text.strip() if response and response.text else None
from pathlib import Path
from PIL import Image

# # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€
image_folder = Path("/workspaces/streamit_sample_app/data/output/image")


pdf_subfolders = [f for f in image_folder.iterdir() if f.is_dir()]

# Use the stem of the PDF file as the folder name (or another variable)


combined_dir = Path("/workspaces/streamit_sample_app/data/output")
# çµåˆç”¨ã®ãƒªã‚¹ãƒˆ
combined_content = []
if pdf_subfolders:
    pdf_subfolder = pdf_subfolders[0]
# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç•ªå·é †ã«å‡¦ç†
image_paths = sorted(pdf_subfolder.glob("*.jpg"), key=lambda x: int(x.stem.split('_')[-1]))  # Sort by number in filename

for image_path in image_paths:
    try:
        # ç”»åƒã‚’é–‹ã„ã¦å‡¦ç†
        img = Image.open(image_path).convert("RGB")  # RGBã«å¤‰æ›
        
        # Geminiã§å‡¦ç†
        markdown_content = process_with_gemini(img)
        
        # å†…å®¹ãŒã‚ã‚Œã°ãƒªã‚¹ãƒˆã«è¿½åŠ 
        if markdown_content:
            combined_content.append(markdown_content)
            print(f"Successfully processed {image_path}")
        else:
            print(f"No content extracted from {image_path}")
    
    except Exception as e:
        print(f"Error processing image {image_path}: {str(e)}")

# æœ€å¾Œã«ã™ã¹ã¦ã®å†…å®¹ã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
if combined_content:
    output_path = combined_dir / "combined_docs.md"
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n\n".join(combined_content))  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆã—ã¦ä¿å­˜
        print(f"All markdown content has been successfully saved to {output_path}")
    except Exception as e:
        print(f"Error saving combined markdown: {str(e)}")
else:
    print("No content was extracted from any images.")



def cosine_similarity(vec_a, vec_b):
    # np.arrayã«å¤‰æ›ã—ã¦ãŠãã¨ã€ãƒ‰ãƒƒãƒˆç©ã‚„ãƒãƒ«ãƒ ã®è¨ˆç®—ãŒç°¡å˜
    a = np.array(vec_a)
    b = np.array(vec_b)

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ = (a Â· b) / (||a|| * ||b||)
    # np.dot() ã§å†…ç©ã‚’è¨ˆç®—ã—ã€np.linalg.norm() ã§ãƒ™ã‚¯ãƒˆãƒ«ã®é•·ã•ã‚’è¨ˆç®—
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

from pathlib import Path
from langchain.text_splitter import CharacterTextSplitter

input_folder = Path("/workspaces/streamit_sample_app/data/output")
output_folder = Path("/workspaces/streamit_sample_app/data/output/chunked_docs")
output_folder.mkdir(parents=True, exist_ok=True)  # Make sure chunked_docs folder exists

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1500,
    chunk_overlap=300
)

# ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãŒ1ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
text_file = input_folder / "combined_docs.md"

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
with open(text_file, "r", encoding="utf-8") as f:
    text = f.read()
    

# ãƒãƒ£ãƒ³ã‚¯åŒ–
chunks = text_splitter.split_text(text)
# `chunked_docs` ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
file_output_folder = output_folder  # Use the chunked_docs folder directly
file_output_folder.mkdir(parents=True, exist_ok=True)

# ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
for i, chunk in enumerate(chunks):
    output_file = file_output_folder / f"{text_file.stem}_chunk{i+1}.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(chunk)
        print(len(chunk))

print(f"âœ… {text_file.name} ã‚’ {len(chunks)} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ«ãƒ€ï¼š{file_output_folder}")


# Define the folder path
output_folder = Path("/workspaces/streamit_sample_app/data/output/chunked_docs")
output_base_folder = Path("/workspaces/streamit_sample_app/data/output/vector_DB")  # Base folder to store embeddings
output_base_folder.mkdir(parents=True, exist_ok=True)

# List to store all embeddings
all_embeddings = []

# Loop through the files in the output folder
for file_path in output_folder.glob("*.txt"):
    # Read the content of the file
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()  # Read the content of the file
    
    print(f"Processing {file_path.name}... (Length of content: {len(content)})")
    
    # Generate the embedding for the content
    response = genai.embed_content(
        model="models/text-embedding-004",
        content=content
    )
    
    # Assuming the response contains the embeddings in a field 'embedding'
    embeddings = response.get('embedding')
    
    if embeddings:
        # Collect embedding data for this file
        embedding_data = {
            "file_name": file_path.name,
            "content": content,
            "embedding": embeddings
        }
        
        # Append the embedding data to the list
        all_embeddings.append(embedding_data)
        
        print(f"âœ… Embeddings for {file_path.name} have been added to the list.")

    # Optional: to avoid hitting rate limits or API restrictions, add a delay between requests
    # sleep(1)  # Adjust the delay as needed (in seconds)

# Save all embeddings to a single JSON file
final_embedding_json_path = output_base_folder / "final_embeddings.json"

# Save to a JSON file inside the base folder
with open(final_embedding_json_path, "w", encoding="utf-8") as json_file:
    json.dump(all_embeddings, json_file, ensure_ascii=False, indent=4)

print(f"âœ… All embeddings have been saved to {final_embedding_json_path}")

def generate_answer(user_query, context):
    # API Rate Limiting
    # time.sleep(5)
    
    model = genai.GenerativeModel("gemini-2.0-flash")
    
    # # system message
    # system_content = (
    #     "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>ã®ã¿ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ã™ã¹ã¦å–å¾—ã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
    #     "<æ³¨æ„ç‚¹>\n"
    #     "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n"
    #     "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆã®æ‰‹ãŒã‹ã‚ŠãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆã¯ã€ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã“ã¨ã€‚\n"
    # )

    # user message
    user_content = (
        "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>ã®ã¿ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«å¯¾ã™ã‚‹ç›´æ¥ã®å›ç­”ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        "å›ç­”ä½œæˆã¯ã€æ–‡å­—æ•°ä¸Šé™54ã‚’è¶…ãˆãªã„ã§ãã ã•ã„ã€‚æ–‡å­—æ•°ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€å›ç­”ã‚’çŸ­ãã—ã¦ãã ã•ã„ã€‚"
        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®æƒ…å ±ã§å›ç­”ä½œæˆãŒå›°é›£ãªå ´åˆã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã€æ³¨æ„ç‚¹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚\n\n"
        "<æ³¨æ„ç‚¹>\n"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‹ã‚‰å›ç­”ã‚’å°ã‘ã€‚\n"
        "- å°æ•°ç¬¬nä½ã‚’å››æ¨äº”å…¥ã™ã‚‹æ™‚ã¯ã€å°æ•°ç¬¬n-1ä½ã¾ã§ã‚’å›ç­”ã¨ã—ãªã•ã„ã€‚\n"
        "- æ•°å€¤ã®å›ç­”ã«ã€Œ,ã€ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
        "- å›ç­”ã¯ç›´æ¥ã®ç­”ãˆã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ã€Œã€‚ã€ã‚„ã€Œã§ã™ã€ãªã©ã‚‚å›ç­”ã«å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
        "- æ•°é‡ã§ç­”ãˆã‚‹å•é¡Œã®å›ç­”ã«ã¯ã€è³ªå•ã«è¨˜è¼‰ã®å˜ä½ã‚’ä½¿ã†ã“ã¨ï¼ˆä¾‹ï¼šâ—‹â—‹å††ã€â—‹â—‹å€ã€â—‹â—‹%ãªã©ï¼‰ã€‚\n"
        "- æ•°é‡ã§ç­”ãˆã‚‹å•é¡Œã®å›ç­”ã«ã¯ã€æ•°é‡ã¨å˜ä½ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹ï¼šâ—‹â—‹å††ã€â—‹â—‹å€ã€â—‹â—‹%ãªã©ï¼‰ã€‚\n"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆã®æ‰‹ãŒã‹ã‚ŠãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆã¯ãã®æ—¨ã‚’ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã“ã¨ã€‚\n\n"
        "<è³ªå•>\n"
        f"{user_query}\n\n"
        "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>\n"
        f"{context}\n\n"
    )

   # Generate response using Gemini model with custom parameters
    # response = model.generate_content(
    #     [user_content],
    #     max_tokens=30000,     # Maximum number of tokens in the response
    #     temperature=0,   # Adjust the temperature for randomness
    #     top_p=0.95,        # Nucleus sampling
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )
    response = model.generate_content([user_content])

    # print("\n===== å›ç­” =====\n")
    # print(response)
    return response.text.strip() if response and response.text else None
def prompt_engineering(user_query):
   
    user_content = (
        "<è³ªå•>\n"
        f"{user_query}\n\n"
        "<è³ªå•>ã‚’æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã€è³ªå•ã‹ã‚‰ç¤¾åã‚’çœã„ãŸã‚Šã€ã‚ã‹ã‚Šã‚„ã™ãã—ãŸã‚Šã—ã¦ãã ã•ã„ã€‚ãªãŠã€æŠ•è³‡DDã®ãŸã‚ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«è³ªå•æ–‡ãŒä½¿ã‚ã‚Œã¾ã™ã€‚"
        "ã™ã§ã«ã€ãã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã¨ä»®å®šã—ã¦ã€æƒ…å ±æ¤œç´¢ã®ãŸã‚ã«è³ªå•æ–‡ã‚’ç°¡æ½”ã‹ã¤å‘ä¸Šã—ã¦ãã ã•ã„ã€‚æ›¸ãæ›ãˆã‚‰ã‚ŒãŸè³ªå•æ–‡ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    messages = [
        # {"role": "system", "content": system_content}, # system message
        {"role": "user", "content": user_content}, # user message
    ]

    model = genai.GenerativeModel("gemini-2.0-flash")
    response = model.generate_content([prompt])
    return response.text.strip() if response and response.text else None

def generate_context(user_query, context):
    # API Rate Limiting
    time.sleep(5)
    
    model = genai.GenerativeModel("gemini-2.0-flash")
    
    # # system message
    # system_content = (
    #     "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>ã®ã¿ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ã™ã¹ã¦å–å¾—ã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
    #     "<æ³¨æ„ç‚¹>\n"
    #     "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n"
    #     "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆã®æ‰‹ãŒã‹ã‚ŠãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆã¯ã€ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã“ã¨ã€‚\n"
    # )

    # user message
    user_content = (
        "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>ã®ã¿ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ã™ã¹ã¦å–å¾—ã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n\n"
        "<æ³¨æ„ç‚¹>\n"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆã®æ‰‹ãŒã‹ã‚ŠãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆã¯ã€ã€Œåˆ†ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹ã“ã¨ã€‚\n\n"
        "<è³ªå•>\n"
        f"{user_query}\n\n"
        "<ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ>\n"
        f"{context}\n\n"
    )

    # messages = [
    #     # {"role": "system", "content": system_content}, # system message
    #     {"role": "user", "content": user_content}, # user message
    # ]

    # Generate response using Gemini model with custom parameters
    # response = model.generate_content(
    #     [user_content],
    #     max_tokens=30000,     # Maximum number of tokens in the response
    #     temperature=0,   # Adjust the temperature for randomness
    #     top_p=0.95,        # Nucleus sampling
    #     frequency_penalty=0,
    #     presence_penalty=0,
    # )
    response = model.generate_content([user_content])

    # print("\n===== å›ç­” =====\n")
    # print(response)
    return response.text.strip() if response and response.text else None


